{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/data/raw_text/train/train_dataset_cleaned.csv')\n",
    "test_df = pd.read_csv('/data/raw_text/test/test_dataset_cleaned.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdfec4",
   "metadata": {},
   "source": [
    "# Text Cleaning and Analysis Pipeline\n",
    "\n",
    "Professional text analysis for meme classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "# Calculate text statistics\n",
    "train_df['text_length'] = train_df['Cleaned_Text'].str.len()\n",
    "train_df['word_count'] = train_df['Cleaned_Text'].str.split().str.len()\n",
    "train_df['avg_word_length'] = train_df['Cleaned_Text'].apply(\n",
    "    lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")\n",
    "\n",
    "# Detect language distribution (Bengali vs English)\n",
    "def detect_language_ratio(text):\n",
    "    if not text or text == '':\n",
    "        return 0, 0, 0\n",
    "    bengali_chars = len(re.findall(r'[\\u0980-\\u09FF]', text))\n",
    "    english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "    total = bengali_chars + english_chars\n",
    "    if total == 0:\n",
    "        return 0, 0, 0\n",
    "    return bengali_chars/total, english_chars/total, total\n",
    "\n",
    "train_df[['bengali_ratio', 'english_ratio', 'char_count']] = train_df['Cleaned_Text'].apply(\n",
    "    lambda x: pd.Series(detect_language_ratio(x))\n",
    ")\n",
    "\n",
    "# Create comprehensive analysis plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Text Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Text length distribution\n",
    "axes[0, 0].hist(train_df['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Text Length (characters)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Text Length Distribution')\n",
    "axes[0, 0].axvline(train_df['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {train_df[\"text_length\"].mean():.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Word count distribution\n",
    "axes[0, 1].hist(train_df['word_count'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].axvline(train_df['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {train_df[\"word_count\"].mean():.1f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Average word length\n",
    "axes[0, 2].hist(train_df['avg_word_length'], bins=30, color='coral', edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Average Word Length')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Average Word Length Distribution')\n",
    "\n",
    "# 4. Text length by label\n",
    "for label in train_df['Label'].unique():\n",
    "    data = train_df[train_df['Label'] == label]['text_length']\n",
    "    axes[1, 0].hist(data, bins=30, alpha=0.6, label=f'Label {label}')\n",
    "axes[1, 0].set_xlabel('Text Length')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Text Length by Label')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 5. Word count by label\n",
    "for label in train_df['Label'].unique():\n",
    "    data = train_df[train_df['Label'] == label]['word_count']\n",
    "    axes[1, 1].hist(data, bins=30, alpha=0.6, label=f'Label {label}')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Word Count by Label')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Language distribution\n",
    "lang_data = pd.DataFrame({\n",
    "    'Bengali': [train_df['bengali_ratio'].mean()],\n",
    "    'English': [train_df['english_ratio'].mean()]\n",
    "})\n",
    "lang_data.T.plot(kind='bar', ax=axes[1, 2], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1, 2].set_xlabel('Language')\n",
    "axes[1, 2].set_ylabel('Ratio')\n",
    "axes[1, 2].set_title('Average Language Distribution')\n",
    "axes[1, 2].set_xticklabels(['Bengali', 'English'], rotation=0)\n",
    "axes[1, 2].legend().remove()\n",
    "\n",
    "# 7. Empty vs non-empty texts\n",
    "empty_count = (train_df['Cleaned_Text'] == '').sum()\n",
    "non_empty_count = (train_df['Cleaned_Text'] != '').sum()\n",
    "axes[2, 0].pie([empty_count, non_empty_count], labels=['Empty', 'Non-empty'], \n",
    "               autopct='%1.1f%%', colors=['#FFB6C1', '#90EE90'])\n",
    "axes[2, 0].set_title('Text Availability')\n",
    "\n",
    "# 8. Label distribution\n",
    "label_counts = train_df['Label'].value_counts()\n",
    "axes[2, 1].bar(label_counts.index, label_counts.values, color='purple', alpha=0.7)\n",
    "axes[2, 1].set_xlabel('Label')\n",
    "axes[2, 1].set_ylabel('Count')\n",
    "axes[2, 1].set_title('Label Distribution')\n",
    "\n",
    "# 9. Language ratio by label\n",
    "for label in train_df['Label'].unique():\n",
    "    data = train_df[train_df['Label'] == label]\n",
    "    axes[2, 2].scatter(data['bengali_ratio'], data['english_ratio'], \n",
    "                       label=f'Label {label}', alpha=0.6, s=30)\n",
    "axes[2, 2].set_xlabel('Bengali Ratio')\n",
    "axes[2, 2].set_ylabel('English Ratio')\n",
    "axes[2, 2].set_title('Language Distribution by Label')\n",
    "axes[2, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT STATISTICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal samples: {len(train_df)}\")\n",
    "print(f\"Empty texts: {empty_count} ({empty_count/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Non-empty texts: {non_empty_count} ({non_empty_count/len(train_df)*100:.2f}%)\")\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {train_df['text_length'].mean():.2f} characters\")\n",
    "print(f\"  Median: {train_df['text_length'].median():.2f} characters\")\n",
    "print(f\"  Std: {train_df['text_length'].std():.2f}\")\n",
    "print(f\"  Min: {train_df['text_length'].min()}\")\n",
    "print(f\"  Max: {train_df['text_length'].max()}\")\n",
    "print(f\"\\nWord Count Statistics:\")\n",
    "print(f\"  Mean: {train_df['word_count'].mean():.2f} words\")\n",
    "print(f\"  Median: {train_df['word_count'].median():.2f} words\")\n",
    "print(f\"  Std: {train_df['word_count'].std():.2f}\")\n",
    "print(f\"\\nLanguage Distribution:\")\n",
    "print(f\"  Average Bengali ratio: {train_df['bengali_ratio'].mean():.2%}\")\n",
    "print(f\"  Average English ratio: {train_df['english_ratio'].mean():.2%}\")\n",
    "print(f\"\\nStatistics by Label:\")\n",
    "for label in sorted(train_df['Label'].unique()):\n",
    "    label_data = train_df[train_df['Label'] == label]\n",
    "    print(f\"\\n  Label {label}:\")\n",
    "    print(f\"    Count: {len(label_data)}\")\n",
    "    print(f\"    Avg text length: {label_data['text_length'].mean():.2f}\")\n",
    "    print(f\"    Avg word count: {label_data['word_count'].mean():.2f}\")\n",
    "    print(f\"    Bengali ratio: {label_data['bengali_ratio'].mean():.2%}\")\n",
    "    print(f\"    English ratio: {label_data['english_ratio'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b9ce1",
   "metadata": {},
   "source": [
    "## Word Frequency and Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7014bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import itertools\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \"\"\"Professional text analysis for word relationships and patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = [str(t) for t in texts if t and str(t).strip() != '']\n",
    "        self.labels = labels\n",
    "        \n",
    "    def get_word_frequencies(self, top_n=30):\n",
    "        \"\"\"Get most frequent words\"\"\"\n",
    "        all_words = []\n",
    "        for text in self.texts:\n",
    "            words = str(text).split()\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_freq = Counter(all_words)\n",
    "        return word_freq.most_common(top_n)\n",
    "    \n",
    "    def get_word_frequencies_by_label(self, label, top_n=20):\n",
    "        \"\"\"Get most frequent words for a specific label\"\"\"\n",
    "        if self.labels is None:\n",
    "            return []\n",
    "        \n",
    "        label_texts = [text for text, lbl in zip(self.texts, self.labels) if lbl == label]\n",
    "        all_words = []\n",
    "        for text in label_texts:\n",
    "            words = str(text).split()\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_freq = Counter(all_words)\n",
    "        return word_freq.most_common(top_n)\n",
    "    \n",
    "    def get_ngrams(self, n=2, top_k=20):\n",
    "        \"\"\"Extract n-grams (word sequences)\"\"\"\n",
    "        ngrams_list = []\n",
    "        for text in self.texts:\n",
    "            words = str(text).split()\n",
    "            if len(words) >= n:\n",
    "                ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "                ngrams_list.extend(ngrams)\n",
    "        \n",
    "        ngram_freq = Counter(ngrams_list)\n",
    "        return ngram_freq.most_common(top_k)\n",
    "    \n",
    "    def get_word_cooccurrence(self, top_words=50, top_pairs=30):\n",
    "        \"\"\"Find words that frequently appear together\"\"\"\n",
    "        # Get top words\n",
    "        word_freq = self.get_word_frequencies(top_words)\n",
    "        top_word_set = set([word for word, _ in word_freq])\n",
    "        \n",
    "        # Find co-occurrences\n",
    "        cooccurrence = defaultdict(int)\n",
    "        for text in self.texts:\n",
    "            words = [w for w in str(text).split() if w in top_word_set]\n",
    "            # Get all pairs of words in the same text\n",
    "            for w1, w2 in itertools.combinations(sorted(set(words)), 2):\n",
    "                cooccurrence[(w1, w2)] += 1\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_cooccurrence = sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_cooccurrence[:top_pairs]\n",
    "    \n",
    "    def get_tfidf_features(self, max_features=100):\n",
    "        \"\"\"Extract TF-IDF features\"\"\"\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get average TF-IDF score for each feature\n",
    "            avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "            feature_scores = list(zip(feature_names, avg_tfidf))\n",
    "            feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return feature_scores\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = TextAnalyzer(\n",
    "    train_df[train_df['Cleaned_Text'] != '']['Cleaned_Text'].values,\n",
    "    train_df[train_df['Cleaned_Text'] != '']['Label'].values\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall word frequencies\n",
    "print(\"\\n1. TOP 30 MOST FREQUENT WORDS (Overall):\")\n",
    "print(\"-\"*80)\n",
    "word_freq = analyzer.get_word_frequencies(30)\n",
    "for i, (word, count) in enumerate(word_freq, 1):\n",
    "    print(f\"{i:2d}. {word:20s} : {count:5d} occurrences\")\n",
    "\n",
    "# Word frequencies by label\n",
    "print(\"\\n2. TOP WORDS BY LABEL:\")\n",
    "print(\"-\"*80)\n",
    "for label in sorted(train_df['Label'].unique()):\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    label_freq = analyzer.get_word_frequencies_by_label(label, 15)\n",
    "    for i, (word, count) in enumerate(label_freq, 1):\n",
    "        print(f\"  {i:2d}. {word:20s} : {count:4d}\")\n",
    "\n",
    "# Bigrams (2-word sequences)\n",
    "print(\"\\n3. TOP 20 BIGRAMS (2-word phrases):\")\n",
    "print(\"-\"*80)\n",
    "bigrams = analyzer.get_ngrams(n=2, top_k=20)\n",
    "for i, (bigram, count) in enumerate(bigrams, 1):\n",
    "    print(f\"{i:2d}. '{bigram:30s}' : {count:4d} occurrences\")\n",
    "\n",
    "# Trigrams (3-word sequences)\n",
    "print(\"\\n4. TOP 15 TRIGRAMS (3-word phrases):\")\n",
    "print(\"-\"*80)\n",
    "trigrams = analyzer.get_ngrams(n=3, top_k=15)\n",
    "for i, (trigram, count) in enumerate(trigrams, 1):\n",
    "    print(f\"{i:2d}. '{trigram:40s}' : {count:4d} occurrences\")\n",
    "\n",
    "# Word co-occurrence\n",
    "print(\"\\n5. TOP 25 WORD CO-OCCURRENCES:\")\n",
    "print(\"-\"*80)\n",
    "print(\"Words that frequently appear together in the same meme:\")\n",
    "cooccurrence = analyzer.get_word_cooccurrence(top_words=50, top_pairs=25)\n",
    "for i, ((word1, word2), count) in enumerate(cooccurrence, 1):\n",
    "    print(f\"{i:2d}. {word1:15s} <-> {word2:15s} : {count:4d} times\")\n",
    "\n",
    "# TF-IDF Analysis\n",
    "print(\"\\n6. TOP 30 TF-IDF FEATURES:\")\n",
    "print(\"-\"*80)\n",
    "print(\"Words/phrases with highest importance (TF-IDF scores):\")\n",
    "tfidf_features = analyzer.get_tfidf_features(100)\n",
    "for i, (feature, score) in enumerate(tfidf_features[:30], 1):\n",
    "    print(f\"{i:2d}. {feature:30s} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
