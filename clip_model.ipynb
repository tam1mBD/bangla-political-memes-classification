{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43746bad",
   "metadata": {},
   "source": [
    "# Building CLIP model to predict memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921ac406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# IMPORTS & CONFIG\n",
    "# =============================\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "DATA_DIR = \"data/raw\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/Train/Train.csv\"\n",
    "TRAIN_IMG_DIR = f\"{DATA_DIR}/Train/Image\"\n",
    "TEST_CSV = f\"{DATA_DIR}/Test/Test.csv\"\n",
    "TEST_IMG_DIR = f\"{DATA_DIR}/Test/Image\"\n",
    "\n",
    "BATCH_SIZE = 16  # Smaller batch size for CLIP\n",
    "EPOCHS = 15\n",
    "LR = 1e-5\n",
    "PATIENCE = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578927b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da404c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2431\n",
      "Validation samples: 429\n",
      "Test samples: 330\n",
      "\n",
      "Class distribution:\n",
      "Label\n",
      "NonPolitical    1706\n",
      "Political        725\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "classes = sorted(train_df[\"Label\"].unique())\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Split into train and validation\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=train_df[\"Label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_split)}\")\n",
    "print(f\"Validation samples: {len(val_split)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_split[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49656f",
   "metadata": {},
   "source": [
    "# Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a76a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: openai/clip-vit-base-patch32\n",
      "Vision embedding dim: 768\n",
      "Projection dim: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded CLIP model: {model_name}\")\n",
    "print(f\"Vision embedding dim: {clip_model.config.vision_config.hidden_size}\")\n",
    "print(f\"Projection dim: {clip_model.config.projection_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afd77c",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f423bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPMemeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, processor, train=True, label2id=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.train = train\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"Image_name\"])\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Fallback for corrupted images\n",
    "            img = Image.new('RGB', (224, 224), (128, 128, 128))\n",
    "        \n",
    "        # Process image with CLIP processor\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "        if self.train:\n",
    "            label = self.label2id[row[\"Label\"]]\n",
    "            return pixel_values, label\n",
    "        \n",
    "        return pixel_values, row[\"Image_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907c70",
   "metadata": {},
   "source": [
    "#  Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f58244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders created!\n"
     ]
    }
   ],
   "source": [
    "train_ds = CLIPMemeDataset(train_split, TRAIN_IMG_DIR, processor, True, label2id)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = CLIPMemeDataset(val_split, TRAIN_IMG_DIR, processor, True, label2id)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_ds = CLIPMemeDataset(test_df, TEST_IMG_DIR, processor, False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Dataloaders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af217e7",
   "metadata": {},
   "source": [
    "# CLIP Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762f92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Classifier created with 63,953,155 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, dropout=0.3):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        \n",
    "        # Freeze CLIP vision encoder initially (optional - can unfreeze later)\n",
    "        for param in self.clip.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        hidden_size = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        # Get image embeddings from CLIP\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = vision_outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Project to CLIP's projection space\n",
    "        image_embeds = self.clip.visual_projection(image_embeds)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(image_embeds)\n",
    "        return logits\n",
    "    \n",
    "    def unfreeze_vision_model(self):\n",
    "        \"\"\"Unfreeze CLIP vision model for fine-tuning\"\"\"\n",
    "        for param in self.clip.vision_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"✓ CLIP vision model unfrozen for fine-tuning\")\n",
    "\n",
    "model = CLIPClassifier(clip_model, num_classes=len(classes)).to(device)\n",
    "print(f\"CLIP Classifier created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb773a",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba6c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.5965, 1.4035])\n",
      "Optimizer: AdamW (lr=1e-05)\n",
      "Scheduler: ReduceLROnPlateau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_175568/2972907319.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_counts = Counter(train_split[\"Label\"].map(label2id))\n",
    "class_weights = torch.tensor(\n",
    "    [1.0 / class_counts[i] for i in range(len(classes))],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "class_weights = class_weights / class_weights.sum() * len(classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"Optimizer: AdamW (lr={LR})\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e614bab",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f9e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 1/15 [Train]: 100%|██████████| 152/152 [07:41<00:00,  3.04s/it, loss=0.1253]\n",
      "Epoch 1/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 1/15 [Val]: 100%|██████████| 27/27 [00:31<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15:\n",
      "  Train Loss: 0.3061, Train Acc: 0.8959\n",
      "  Val Loss: 0.3495, Val Acc: 0.8928, Val F1: 0.8656\n",
      "  ✓ Best model saved! (Val F1: 0.8656)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 2/15 [Train]: 100%|██████████| 152/152 [07:45<00:00,  3.06s/it, loss=0.0623]\n",
      "Epoch 2/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 2/15 [Val]: 100%|██████████| 27/27 [00:24<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15:\n",
      "  Train Loss: 0.1699, Train Acc: 0.9535\n",
      "  Val Loss: 0.3530, Val Acc: 0.8881, Val F1: 0.8619\n",
      "  No improvement (1/5)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 3/15 [Train]: 100%|██████████| 152/152 [07:33<00:00,  2.98s/it, loss=0.1172]\n",
      "Epoch 3/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 3/15 [Val]: 100%|██████████| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15:\n",
      "  Train Loss: 0.0867, Train Acc: 0.9811\n",
      "  Val Loss: 0.3976, Val Acc: 0.9021, Val F1: 0.8779\n",
      "  ✓ Best model saved! (Val F1: 0.8779)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 4/15 [Train]: 100%|██████████| 152/152 [07:46<00:00,  3.07s/it, loss=0.0041]\n",
      "Epoch 4/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 4/15 [Val]: 100%|██████████| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15:\n",
      "  Train Loss: 0.0361, Train Acc: 0.9947\n",
      "  Val Loss: 0.6086, Val Acc: 0.9044, Val F1: 0.8779\n",
      "  No improvement (1/5)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 5/15 [Train]: 100%|██████████| 152/152 [07:44<00:00,  3.06s/it, loss=0.0014]\n",
      "Epoch 5/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 5/15 [Val]: 100%|██████████| 27/27 [00:26<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15:\n",
      "  Train Loss: 0.0166, Train Acc: 0.9967\n",
      "  Val Loss: 0.7929, Val Acc: 0.9021, Val F1: 0.8746\n",
      "  No improvement (2/5)\n",
      "--------------------------------------------------\n",
      "✓ CLIP vision model unfrozen for fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 6/15 [Train]: 100%|██████████| 152/152 [08:01<00:00,  3.17s/it, loss=0.0002]\n",
      "Epoch 6/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 6/15 [Val]: 100%|██████████| 27/27 [00:27<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15:\n",
      "  Train Loss: 0.0089, Train Acc: 0.9984\n",
      "  Val Loss: 0.8827, Val Acc: 0.8998, Val F1: 0.8766\n",
      "  No improvement (3/5)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 7/15 [Train]: 100%|██████████| 152/152 [08:04<00:00,  3.19s/it, loss=0.0001]\n",
      "Epoch 7/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 7/15 [Val]: 100%|██████████| 27/27 [00:24<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.0042, Val Acc: 0.8951, Val F1: 0.8695\n",
      "  No improvement (4/5)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 8/15 [Train]: 100%|██████████| 152/152 [07:45<00:00,  3.06s/it, loss=0.0000]\n",
      "Epoch 8/15 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipykernel_175568/1999252382.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
      "Epoch 8/15 [Val]: 100%|██████████| 27/27 [00:25<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/15:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.0708, Val Acc: 0.8951, Val F1: 0.8695\n",
      "  No improvement (5/5)\n",
      "\n",
      "Early stopping triggered after 8 epochs\n",
      "\n",
      "Training finished! Best Val F1: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Optional: Unfreeze CLIP after a few epochs\n",
    "    if epoch == 5:\n",
    "        model.unfreeze_vision_model()\n",
    "        # Recreate optimizer with lower LR for fine-tuning\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR/10, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for pixel_values, labels in pbar:\n",
    "        pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "            outputs = model(pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pixel_values, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                outputs = model(pixel_values)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'models/clip_model.pth')\n",
    "        print(f\"  ✓ Best model saved! (Val F1: {val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nTraining finished! Best Val F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161fef6",
   "metadata": {},
   "source": [
    "# Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final validation: 100%|██████████| 27/27 [00:24<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL VALIDATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "NonPolitical       0.91      0.96      0.93       301\n",
      "   Political       0.89      0.77      0.82       128\n",
      "\n",
      "    accuracy                           0.90       429\n",
      "   macro avg       0.90      0.86      0.88       429\n",
      "weighted avg       0.90      0.90      0.90       429\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Confusion matrix saved as 'confusion_matrix_clip.png'\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/clip_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get final validation predictions\n",
    "val_preds, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for pixel_values, labels in tqdm(val_loader, desc=\"Final validation\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=[id2label[i] for i in range(len(classes))]))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[id2label[i] for i in range(len(classes))],\n",
    "            yticklabels=[id2label[i] for i in range(len(classes))])\n",
    "plt.title('Confusion Matrix - CLIP Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/confusion_matrix_clip.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Confusion matrix saved as 'assets/confusion_matrix_clip.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1a35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training history\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# # Loss plot\n",
    "# axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "# axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "# axes[0].set_xlabel('Epoch')\n",
    "# axes[0].set_ylabel('Loss')\n",
    "# axes[0].set_title('Training and Validation Loss')\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True)\n",
    "\n",
    "# # Accuracy plot\n",
    "# axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "# axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
    "# axes[1].set_xlabel('Epoch')\n",
    "# axes[1].set_ylabel('Accuracy')\n",
    "# axes[1].set_title('Training and Validation Accuracy')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True)\n",
    "\n",
    "# # F1 Score plot\n",
    "# axes[2].plot(history['val_f1'], label='Val F1 Score', marker='d', color='green')\n",
    "# axes[2].set_xlabel('Epoch')\n",
    "# axes[2].set_ylabel('F1 Score')\n",
    "# axes[2].set_title('Validation F1 Score')\n",
    "# axes[2].legend()\n",
    "# axes[2].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('training_history_clip.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"✓ Training history saved as 'training_history_clip.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0798b",
   "metadata": {},
   "source": [
    "# Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4abf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:19<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV saved at: submissions/submission_clip.csv\n",
      "     Image_name         Label\n",
      "0  test0001.jpg  NonPolitical\n",
      "1  test0002.jpg  NonPolitical\n",
      "2  test0003.jpg  NonPolitical\n",
      "3  test0004.jpg  NonPolitical\n",
      "4  test0005.jpg     Political\n",
      "\n",
      "Prediction distribution:\n",
      "Label\n",
      "NonPolitical    234\n",
      "Political        96\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pixel_values, names in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values)\n",
    "        predicted = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        preds.extend([id2label[p] for p in predicted])\n",
    "        image_names.extend(names)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Image_name\": image_names,\n",
    "    \"Label\": preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_csv = \"submissions/submission_clip.csv\"\n",
    "submission_df.to_csv(submission_csv, index=False)\n",
    "\n",
    "print(f\"Submission CSV saved at: {submission_csv}\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_df['Label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
