{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43746bad",
   "metadata": {},
   "source": [
    "# Building CLIP model to predict memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ac406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# IMPORTS & CONFIG\n",
    "# =============================\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "DATA_DIR = \"/data\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/Train/Train.csv\"\n",
    "TRAIN_IMG_DIR = f\"{DATA_DIR}/Train/Image\"\n",
    "TEST_CSV = f\"{DATA_DIR}/Test/Test.csv\"\n",
    "TEST_IMG_DIR = f\"{DATA_DIR}/Test/Image\"\n",
    "\n",
    "BATCH_SIZE = 16  # Smaller batch size for CLIP\n",
    "EPOCHS = 15\n",
    "LR = 1e-5\n",
    "PATIENCE = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578927b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da404c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD DATA\n",
    "# =============================\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "classes = sorted(train_df[\"Label\"].unique())\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Split into train and validation\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=train_df[\"Label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_split)}\")\n",
    "print(f\"Validation samples: {len(val_split)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_split[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49656f",
   "metadata": {},
   "source": [
    "# Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD CLIP MODEL\n",
    "# =============================\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded CLIP model: {model_name}\")\n",
    "print(f\"Vision embedding dim: {clip_model.config.vision_config.hidden_size}\")\n",
    "print(f\"Projection dim: {clip_model.config.projection_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afd77c",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f423bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# DATASET CLASS\n",
    "# =============================\n",
    "class CLIPMemeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, processor, train=True, label2id=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.train = train\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"Image_name\"])\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Fallback for corrupted images\n",
    "            img = Image.new('RGB', (224, 224), (128, 128, 128))\n",
    "        \n",
    "        # Process image with CLIP processor\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "        if self.train:\n",
    "            label = self.label2id[row[\"Label\"]]\n",
    "            return pixel_values, label\n",
    "        \n",
    "        return pixel_values, row[\"Image_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907c70",
   "metadata": {},
   "source": [
    "#  Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# DATALOADERS\n",
    "# =============================\n",
    "train_ds = CLIPMemeDataset(train_split, TRAIN_IMG_DIR, processor, True, label2id)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = CLIPMemeDataset(val_split, TRAIN_IMG_DIR, processor, True, label2id)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_ds = CLIPMemeDataset(test_df, TEST_IMG_DIR, processor, False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Dataloaders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af217e7",
   "metadata": {},
   "source": [
    "# CLIP Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# CLIP CLASSIFIER\n",
    "# =============================\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, dropout=0.3):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        \n",
    "        # Freeze CLIP vision encoder initially (optional - can unfreeze later)\n",
    "        for param in self.clip.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        hidden_size = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        # Get image embeddings from CLIP\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = vision_outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Project to CLIP's projection space\n",
    "        image_embeds = self.clip.visual_projection(image_embeds)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(image_embeds)\n",
    "        return logits\n",
    "    \n",
    "    def unfreeze_vision_model(self):\n",
    "        \"\"\"Unfreeze CLIP vision model for fine-tuning\"\"\"\n",
    "        for param in self.clip.vision_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"✓ CLIP vision model unfrozen for fine-tuning\")\n",
    "\n",
    "model = CLIPClassifier(clip_model, num_classes=len(classes)).to(device)\n",
    "print(f\"CLIP Classifier created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb773a",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TRAINING SETUP\n",
    "# =============================\n",
    "# Calculate class weights\n",
    "class_counts = Counter(train_split[\"Label\"].map(label2id))\n",
    "class_weights = torch.tensor(\n",
    "    [1.0 / class_counts[i] for i in range(len(classes))],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "class_weights = class_weights / class_weights.sum() * len(classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"Optimizer: AdamW (lr={LR})\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e614bab",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TRAINING LOOP\n",
    "# =============================\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Optional: Unfreeze CLIP after a few epochs\n",
    "    if epoch == 5:\n",
    "        model.unfreeze_vision_model()\n",
    "        # Recreate optimizer with lower LR for fine-tuning\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR/10, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for pixel_values, labels in pbar:\n",
    "        pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "            outputs = model(pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pixel_values, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                outputs = model(pixel_values)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_clip_model.pth')\n",
    "        print(f\"  ✓ Best model saved! (Val F1: {val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nTraining finished! Best Val F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161fef6",
   "metadata": {},
   "source": [
    "# Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_clip_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get final validation predictions\n",
    "val_preds, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for pixel_values, labels in tqdm(val_loader, desc=\"Final validation\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=[id2label[i] for i in range(len(classes))]))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[id2label[i] for i in range(len(classes))],\n",
    "            yticklabels=[id2label[i] for i in range(len(classes))])\n",
    "plt.title('Confusion Matrix - CLIP Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_clip.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Confusion matrix saved as 'confusion_matrix_clip.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# F1 Score plot\n",
    "axes[2].plot(history['val_f1'], label='Val F1 Score', marker='d', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('Validation F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_clip.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training history saved as 'training_history_clip.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0798b",
   "metadata": {},
   "source": [
    "# Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4abf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# PREDICTION & SUBMISSION\n",
    "# =============================\n",
    "model.eval()\n",
    "preds = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pixel_values, names in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values)\n",
    "        predicted = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        preds.extend([id2label[p] for p in predicted])\n",
    "        image_names.extend(names)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Image_name\": image_names,\n",
    "    \"Label\": preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_csv = \"submission_clip.csv\"\n",
    "submission_df.to_csv(submission_csv, index=False)\n",
    "\n",
    "print(f\"Submission CSV saved at: {submission_csv}\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_df['Label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
