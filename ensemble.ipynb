{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5692ffff",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2f25b",
   "metadata": {},
   "source": [
    "#  Load Pre-trained Models\n",
    "\n",
    "## 1. Text Model (mBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD MBERT MODEL\n",
    "# =============================\n",
    "print(\"Loading mBERT model...\")\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "mbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "mbert_model.load_state_dict(torch.load(MBERT_MODEL_PATH, map_location=device))\n",
    "mbert_model = mbert_model.to(device)\n",
    "mbert_model.eval()\n",
    "\n",
    "print(\"âœ“ mBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8c297",
   "metadata": {},
   "source": [
    "## 2. Visual Model (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD CLIP MODEL\n",
    "# =============================\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# CLIP Classifier class (same as training)\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, dropout=0.3):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        hidden_size = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = vision_outputs.pooler_output\n",
    "        image_embeds = self.clip.visual_projection(image_embeds)\n",
    "        logits = self.classifier(image_embeds)\n",
    "        return logits\n",
    "\n",
    "# Load CLIP\n",
    "clip_base = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPClassifier(clip_base, num_classes=2)\n",
    "\n",
    "# Load trained weights\n",
    "clip_model.load_state_dict(torch.load(CLIP_MODEL_PATH, map_location=device))\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"âœ“ CLIP model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ee682",
   "metadata": {},
   "source": [
    "#  Generate Predictions from Base Models\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# PREDICTION FUNCTIONS\n",
    "# =============================\n",
    "\n",
    "def get_text_predictions(texts, model, tokenizer, device, batch_size=16):\n",
    "    \"\"\"Get probability predictions from text model (mBERT)\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Text predictions\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "def get_image_predictions(image_names, img_dir, model, processor, device, batch_size=16):\n",
    "    \"\"\"Get probability predictions from visual model (CLIP)\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(image_names), batch_size), desc=\"Image predictions\"):\n",
    "            batch_names = image_names[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # Load images\n",
    "            for img_name in batch_names:\n",
    "                img_path = os.path.join(img_dir, img_name)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                except:\n",
    "                    img = Image.new('RGB', (224, 224), (128, 128, 128))\n",
    "                batch_images.append(img)\n",
    "            \n",
    "            # Process images\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(pixel_values)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "print(\"Prediction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5508f",
   "metadata": {},
   "source": [
    "## Generate Validation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# GENERATE VALIDATION PREDICTIONS\n",
    "# =============================\n",
    "print(\"Generating predictions on validation set...\\n\")\n",
    "\n",
    "# Text predictions\n",
    "val_texts = val_split['Processed_Text'].fillna('').tolist()\n",
    "text_proba_val = get_text_predictions(val_texts, mbert_model, mbert_tokenizer, device)\n",
    "\n",
    "# Image predictions\n",
    "val_images = val_split['Image_name'].tolist()\n",
    "image_proba_val = get_image_predictions(val_images, TRAIN_IMG_DIR, clip_model, clip_processor, device)\n",
    "\n",
    "# True labels\n",
    "y_val = (val_split['Label'] == 'Political').astype(int).values\n",
    "\n",
    "print(f\"\\nValidation predictions generated!\")\n",
    "print(f\"Text probabilities shape: {text_proba_val.shape}\")\n",
    "print(f\"Image probabilities shape: {image_proba_val.shape}\")\n",
    "print(f\"Labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e6cb7",
   "metadata": {},
   "source": [
    "## Analyze Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# BASE MODEL PERFORMANCE\n",
    "# =============================\n",
    "text_preds_val = np.argmax(text_proba_val, axis=1)\n",
    "image_preds_val = np.argmax(image_proba_val, axis=1)\n",
    "\n",
    "text_acc = accuracy_score(y_val, text_preds_val)\n",
    "text_f1 = f1_score(y_val, text_preds_val, average='macro')\n",
    "\n",
    "image_acc = accuracy_score(y_val, image_preds_val)\n",
    "image_f1 = f1_score(y_val, image_preds_val, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL PERFORMANCE ON VALIDATION SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nText Model (mBERT):\")\n",
    "print(f\"  Accuracy: {text_acc:.4f}\")\n",
    "print(f\"  F1 Score: {text_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nImage Model (CLIP):\")\n",
    "print(f\"  Accuracy: {image_acc:.4f}\")\n",
    "print(f\"  F1 Score: {image_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6add5",
   "metadata": {},
   "source": [
    "#  Engineer Meta-Features for Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# FEATURE ENGINEERING FOR META-MODEL\n",
    "# =============================\n",
    "\n",
    "def create_stacking_features(text_proba, image_proba):\n",
    "    \"\"\"\n",
    "    Create engineered features for meta-model\n",
    "    \n",
    "    Features:\n",
    "    1-2: Raw text probabilities [NonPol, Pol]\n",
    "    3-4: Raw image probabilities [NonPol, Pol]\n",
    "    5: Text confidence (max probability)\n",
    "    6: Image confidence (max probability)\n",
    "    7: Models agreement (binary: do they predict same class?)\n",
    "    8: Disagreement magnitude (absolute difference in Political prob)\n",
    "    9: Both predict Political (product of Political probs)\n",
    "    10: Both predict NonPolitical (product of NonPolitical probs)\n",
    "    11: Average Political probability\n",
    "    12: Max Political probability\n",
    "    13: Min Political probability\n",
    "    14: Difference (Text Political - Image Political)\n",
    "    \"\"\"\n",
    "    features = np.column_stack([\n",
    "        # Base probabilities (4 features)\n",
    "        text_proba[:, 0],              # Text: prob NonPolitical\n",
    "        text_proba[:, 1],              # Text: prob Political\n",
    "        image_proba[:, 0],             # Image: prob NonPolitical\n",
    "        image_proba[:, 1],             # Image: prob Political\n",
    "        \n",
    "        # Confidence features (2 features)\n",
    "        np.max(text_proba, axis=1),    # Text confidence\n",
    "        np.max(image_proba, axis=1),   # Image confidence\n",
    "        \n",
    "        # Agreement features (2 features)\n",
    "        (np.argmax(text_proba, axis=1) == np.argmax(image_proba, axis=1)).astype(float),  # Agreement\n",
    "        np.abs(text_proba[:, 1] - image_proba[:, 1]),  # Disagreement magnitude\n",
    "        \n",
    "        # Interaction features (2 features)\n",
    "        text_proba[:, 1] * image_proba[:, 1],  # Both say Political\n",
    "        text_proba[:, 0] * image_proba[:, 0],  # Both say NonPolitical\n",
    "        \n",
    "        # Statistical features (4 features)\n",
    "        (text_proba[:, 1] + image_proba[:, 1]) / 2,  # Average Political prob\n",
    "        np.maximum(text_proba[:, 1], image_proba[:, 1]),  # Max Political prob\n",
    "        np.minimum(text_proba[:, 1], image_proba[:, 1]),  # Min Political prob\n",
    "        text_proba[:, 1] - image_proba[:, 1],  # Difference (Text - Image)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create validation features\n",
    "X_val_stacking = create_stacking_features(text_proba_val, image_proba_val)\n",
    "\n",
    "print(f\"Stacking features shape: {X_val_stacking.shape}\")\n",
    "print(f\"Total features: {X_val_stacking.shape[1]}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Base probabilities: 4\")\n",
    "print(f\"  - Confidence features: 2\")\n",
    "print(f\"  - Agreement features: 2\")\n",
    "print(f\"  - Interaction features: 2\")\n",
    "print(f\"  - Statistical features: 4\")\n",
    "print(f\"  - Total: 14 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974bd5d0",
   "metadata": {},
   "source": [
    "# Train Meta-Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TRAIN META-MODEL WITH GRID SEARCH\n",
    "# =============================\n",
    "print(\"Training Logistic Regression meta-model with Grid Search...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'class_weight': ['balanced', None],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Create base meta-model\n",
    "meta_model_base = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    meta_model_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_val_stacking, y_val)\n",
    "\n",
    "# Best model\n",
    "meta_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36478202",
   "metadata": {},
   "source": [
    "## Analyze Meta-Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc060a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# META-MODEL PERFORMANCE\n",
    "# =============================\n",
    "ensemble_preds_val = meta_model.predict(X_val_stacking)\n",
    "ensemble_proba_val = meta_model.predict_proba(X_val_stacking)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_val, ensemble_preds_val)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_preds_val, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEnsemble (Stacking):\")\n",
    "print(f\"  Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"  F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nText Model:     Acc={text_acc:.4f}, F1={text_f1:.4f}\")\n",
    "print(f\"Image Model:    Acc={image_acc:.4f}, F1={image_f1:.4f}\")\n",
    "print(f\"Ensemble:       Acc={ensemble_acc:.4f}, F1={ensemble_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMPROVEMENTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy improvement over Text:  {(ensemble_acc - text_acc)*100:+.2f}%\")\n",
    "print(f\"Accuracy improvement over Image: {(ensemble_acc - image_acc)*100:+.2f}%\")\n",
    "print(f\"F1 improvement over Text:        {(ensemble_f1 - text_f1)*100:+.2f}%\")\n",
    "print(f\"F1 improvement over Image:       {(ensemble_f1 - image_f1)*100:+.2f}%\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc19bf",
   "metadata": {},
   "source": [
    "# Late Fusion\n",
    "- Between first model build with deep learing and stacking model build with logistic regresion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf67a9",
   "metadata": {},
   "source": [
    "# =============================\n",
    "# LATE FUSION: Combining Neural Network & Ensemble Predictions\n",
    "# =============================\n",
    "print(\"=\"*60)\n",
    "print(\"LATE FUSION IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCombining predictions from:\")\n",
    "print(\"  1. Deep Learning Model (Neural Network on text features)\")\n",
    "print(\"  2. Stacking Ensemble (mBERT + CLIP with Logistic Regression)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42dfbd",
   "metadata": {},
   "source": [
    "### STEP 1: Load Neural Network Model & Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 1: Loading Neural Network model...\\n\")\n",
    "\n",
    "# Load the trained neural network\n",
    "from tensorflow import keras\n",
    "nn_model = keras.models.load_model('neural_network_model.h5')\n",
    "print(\"âœ“ Neural Network model loaded!\")\n",
    "\n",
    "# Get validation predictions from Neural Network\n",
    "# Use the same validation set as the ensemble\n",
    "val_features_nn = val_split[['political_specific_count', 'political_specific_ratio']].fillna(0)\n",
    "val_features_nn_scaled = scaler.transform(val_features_nn)\n",
    "\n",
    "nn_proba_val = nn_model.predict(val_features_nn_scaled, verbose=0)\n",
    "nn_preds_val = (nn_proba_val > 0.5).astype(int).flatten()\n",
    "\n",
    "# Get test predictions from Neural Network\n",
    "test_features_nn = test_text_df[['political_specific_count', 'political_specific_ratio']].fillna(0)\n",
    "test_features_nn_scaled = scaler.transform(test_features_nn)\n",
    "\n",
    "nn_proba_test = nn_model.predict(test_features_nn_scaled, verbose=0)\n",
    "nn_preds_test = (nn_proba_test > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nNeural Network predictions generated!\")\n",
    "print(f\"  Val shape: {nn_proba_val.shape}\")\n",
    "print(f\"  Test shape: {nn_proba_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fd549",
   "metadata": {},
   "source": [
    "### STEP 2: Get Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 2: Using existing Ensemble predictions...\")\n",
    "print(f\"  Ensemble val predictions: {ensemble_proba_val.shape}\")\n",
    "print(f\"  Ensemble test predictions: {ensemble_proba_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee776e52",
   "metadata": {},
   "source": [
    "### STEP 3: Analyze Individual Model Performance on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "nn_acc_val = accuracy_score(y_val, nn_preds_val)\n",
    "nn_f1_val = f1_score(y_val, nn_preds_val, average='macro')\n",
    "\n",
    "print(f\"\\n1. Neural Network (Text Features):\")\n",
    "print(f\"     Accuracy: {nn_acc_val:.4f}\")\n",
    "print(f\"     F1 Score: {nn_f1_val:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Text Model (mBERT):\")\n",
    "print(f\"     Accuracy: {text_acc:.4f}\")\n",
    "print(f\"     F1 Score: {text_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Image Model (CLIP):\")\n",
    "print(f\"     Accuracy: {image_acc:.4f}\")\n",
    "print(f\"     F1 Score: {image_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n4. Ensemble (Stacking):\")\n",
    "print(f\"     Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"     F1 Score: {ensemble_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3b686",
   "metadata": {},
   "source": [
    "### STEP 4: Late Fusion Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATE FUSION STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert probabilities to same format (both should be [N, 2] shape)\n",
    "# Neural Network outputs [N, 1], so we need to convert\n",
    "nn_proba_val_2d = np.column_stack([1 - nn_proba_val, nn_proba_val])\n",
    "nn_proba_test_2d = np.column_stack([1 - nn_proba_test, nn_proba_test])\n",
    "\n",
    "# Method 1: Simple Average (Equal Weights)\n",
    "print(\"\\n1. SIMPLE AVERAGE (Equal Weights)\")\n",
    "avg_proba_val = (nn_proba_val_2d + ensemble_proba_val) / 2\n",
    "avg_preds_val = np.argmax(avg_proba_val, axis=1)\n",
    "\n",
    "avg_acc = accuracy_score(y_val, avg_preds_val)\n",
    "avg_f1 = f1_score(y_val, avg_preds_val, average='macro')\n",
    "print(f\"   Validation Accuracy: {avg_acc:.4f}\")\n",
    "print(f\"   Validation F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "# Method 2: Weighted Average (Based on Validation Performance)\n",
    "print(\"\\n2. WEIGHTED AVERAGE (Performance-based)\")\n",
    "# Weights based on F1 scores\n",
    "total_f1 = nn_f1_val + ensemble_f1\n",
    "w_nn = nn_f1_val / total_f1\n",
    "w_ensemble = ensemble_f1 / total_f1\n",
    "\n",
    "weighted_proba_val = w_nn * nn_proba_val_2d + w_ensemble * ensemble_proba_val\n",
    "weighted_preds_val = np.argmax(weighted_proba_val, axis=1)\n",
    "\n",
    "weighted_acc = accuracy_score(y_val, weighted_preds_val)\n",
    "weighted_f1 = f1_score(y_val, weighted_preds_val, average='macro')\n",
    "print(f\"   Weights: NN={w_nn:.3f}, Ensemble={w_ensemble:.3f}\")\n",
    "print(f\"   Validation Accuracy: {weighted_acc:.4f}\")\n",
    "print(f\"   Validation F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Method 3: Max Probability Voting\n",
    "print(\"\\n3. MAX PROBABILITY VOTING\")\n",
    "max_nn = np.max(nn_proba_val_2d, axis=1)\n",
    "max_ensemble = np.max(ensemble_proba_val, axis=1)\n",
    "\n",
    "max_vote_preds_val = np.where(\n",
    "    max_nn > max_ensemble,\n",
    "    np.argmax(nn_proba_val_2d, axis=1),\n",
    "    np.argmax(ensemble_proba_val, axis=1)\n",
    ")\n",
    "\n",
    "max_vote_acc = accuracy_score(y_val, max_vote_preds_val)\n",
    "max_vote_f1 = f1_score(y_val, max_vote_preds_val, average='macro')\n",
    "print(f\"   Validation Accuracy: {max_vote_acc:.4f}\")\n",
    "print(f\"   Validation F1 Score: {max_vote_f1:.4f}\")\n",
    "\n",
    "# Method 4: Majority Voting\n",
    "print(\"\\n4. MAJORITY VOTING\")\n",
    "nn_vote = nn_preds_val\n",
    "ensemble_vote = ensemble_preds_val\n",
    "\n",
    "majority_preds_val = np.where(\n",
    "    nn_vote == ensemble_vote,\n",
    "    nn_vote,  # If they agree, use their prediction\n",
    "    ensemble_vote  # If they disagree, trust ensemble\n",
    ")\n",
    "\n",
    "majority_acc = accuracy_score(y_val, majority_preds_val)\n",
    "majority_f1 = f1_score(y_val, majority_preds_val, average='macro')\n",
    "print(f\"   Validation Accuracy: {majority_acc:.4f}\")\n",
    "print(f\"   Validation F1 Score: {majority_f1:.4f}\")\n",
    "\n",
    "# Method 5: Learned Fusion (Train another meta-model)\n",
    "print(\"\\n5. LEARNED FUSION (Meta-Logistic Regression)\")\n",
    "fusion_features_val = np.column_stack([\n",
    "    nn_proba_val_2d[:, 1],  # NN Political probability\n",
    "    ensemble_proba_val[:, 1],  # Ensemble Political probability\n",
    "    nn_proba_val_2d[:, 1] * ensemble_proba_val[:, 1],  # Product\n",
    "    np.abs(nn_proba_val_2d[:, 1] - ensemble_proba_val[:, 1]),  # Difference\n",
    "    np.max(nn_proba_val_2d, axis=1),  # NN confidence\n",
    "    np.max(ensemble_proba_val, axis=1),  # Ensemble confidence\n",
    "])\n",
    "\n",
    "fusion_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "fusion_model.fit(fusion_features_val, y_val)\n",
    "fusion_preds_val = fusion_model.predict(fusion_features_val)\n",
    "\n",
    "fusion_acc = accuracy_score(y_val, fusion_preds_val)\n",
    "fusion_f1 = f1_score(y_val, fusion_preds_val, average='macro')\n",
    "print(f\"   Validation Accuracy: {fusion_acc:.4f}\")\n",
    "print(f\"   Validation F1 Score: {fusion_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a35bc6",
   "metadata": {},
   "source": [
    "### STEP 5: Compare All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Neural Network',\n",
    "        'Ensemble (Stacking)',\n",
    "        'Late Fusion: Average',\n",
    "        'Late Fusion: Weighted',\n",
    "        'Late Fusion: Max Prob',\n",
    "        'Late Fusion: Majority',\n",
    "        'Late Fusion: Learned'\n",
    "    ],\n",
    "    'Accuracy': [nn_acc_val, ensemble_acc, avg_acc, weighted_acc, max_vote_acc, majority_acc, fusion_acc],\n",
    "    'F1 Score': [nn_f1_val, ensemble_f1, avg_f1, weighted_f1, max_vote_f1, majority_f1, fusion_f1]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Find best method\n",
    "best_method_idx = results['F1 Score'].idxmax()\n",
    "best_method_name = results.iloc[best_method_idx]['Model']\n",
    "best_f1 = results.iloc[best_method_idx]['F1 Score']\n",
    "\n",
    "print(f\"\\nðŸ† BEST METHOD: {best_method_name} (F1: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ad1c7",
   "metadata": {},
   "source": [
    "### STEP 6: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart for comparison\n",
    "results_sorted = results.sort_values('F1 Score', ascending=True)\n",
    "colors = ['#3498db' if 'Late Fusion' not in m else '#e74c3c' for m in results_sorted['Model']]\n",
    "\n",
    "ax1.barh(results_sorted['Model'], results_sorted['F1 Score'], color=colors)\n",
    "ax1.set_xlabel('F1 Score')\n",
    "ax1.set_title('Model Performance Comparison')\n",
    "ax1.axvline(x=ensemble_f1, color='green', linestyle='--', linewidth=2, label='Ensemble Baseline')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Confusion matrix for best method\n",
    "if best_method_name == 'Late Fusion: Average':\n",
    "    best_preds = avg_preds_val\n",
    "elif best_method_name == 'Late Fusion: Weighted':\n",
    "    best_preds = weighted_preds_val\n",
    "elif best_method_name == 'Late Fusion: Max Prob':\n",
    "    best_preds = max_vote_preds_val\n",
    "elif best_method_name == 'Late Fusion: Majority':\n",
    "    best_preds = majority_preds_val\n",
    "else:\n",
    "    best_preds = fusion_preds_val\n",
    "\n",
    "cm = confusion_matrix(y_val, best_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', ax=ax2,\n",
    "            xticklabels=['NonPol', 'Pol'], yticklabels=['NonPol', 'Pol'])\n",
    "ax2.set_title(f'Confusion Matrix - {best_method_name}')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('late_fusion_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Comparison plot saved as 'late_fusion_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ecc36",
   "metadata": {},
   "source": [
    "### STEP 7: Generate Test Predictions with Best Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab99ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING TEST PREDICTIONS WITH BEST METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply best method to test set\n",
    "if best_method_name == 'Late Fusion: Average':\n",
    "    test_proba_late_fusion = (nn_proba_test_2d + ensemble_proba_test) / 2\n",
    "    \n",
    "elif best_method_name == 'Late Fusion: Weighted':\n",
    "    test_proba_late_fusion = w_nn * nn_proba_test_2d + w_ensemble * ensemble_proba_test\n",
    "    \n",
    "elif best_method_name == 'Late Fusion: Max Prob':\n",
    "    max_nn_test = np.max(nn_proba_test_2d, axis=1)\n",
    "    max_ensemble_test = np.max(ensemble_proba_test, axis=1)\n",
    "    test_preds_late_fusion = np.where(\n",
    "        max_nn_test > max_ensemble_test,\n",
    "        np.argmax(nn_proba_test_2d, axis=1),\n",
    "        np.argmax(ensemble_proba_test, axis=1)\n",
    "    )\n",
    "    \n",
    "elif best_method_name == 'Late Fusion: Majority':\n",
    "    nn_vote_test = nn_preds_test\n",
    "    ensemble_vote_test = np.argmax(ensemble_proba_test, axis=1)\n",
    "    test_preds_late_fusion = np.where(\n",
    "        nn_vote_test == ensemble_vote_test,\n",
    "        nn_vote_test,\n",
    "        ensemble_vote_test\n",
    "    )\n",
    "    \n",
    "else:  # Learned Fusion\n",
    "    fusion_features_test = np.column_stack([\n",
    "        nn_proba_test_2d[:, 1],\n",
    "        ensemble_proba_test[:, 1],\n",
    "        nn_proba_test_2d[:, 1] * ensemble_proba_test[:, 1],\n",
    "        np.abs(nn_proba_test_2d[:, 1] - ensemble_proba_test[:, 1]),\n",
    "        np.max(nn_proba_test_2d, axis=1),\n",
    "        np.max(ensemble_proba_test, axis=1),\n",
    "    ])\n",
    "    test_preds_late_fusion = fusion_model.predict(fusion_features_test)\n",
    "\n",
    "# If we have probabilities, convert to predictions\n",
    "if 'test_proba_late_fusion' in locals():\n",
    "    test_preds_late_fusion = np.argmax(test_proba_late_fusion, axis=1)\n",
    "\n",
    "print(f\"\\nâœ“ Using: {best_method_name}\")\n",
    "print(f\"  Test predictions generated: {len(test_preds_late_fusion)}\")\n",
    "print(f\"  Political: {(test_preds_late_fusion == 1).sum()}\")\n",
    "print(f\"  NonPolitical: {(test_preds_late_fusion == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2aa90",
   "metadata": {},
   "source": [
    "### STEP 8: Create Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_late_fusion = pd.DataFrame({\n",
    "    'Image_name': test_text_df['Image_name'],\n",
    "    'Label': ['Political' if pred == 1 else 'NonPolitical' for pred in test_preds_late_fusion]\n",
    "})\n",
    "\n",
    "submission_late_fusion.to_csv('submission_late_fusion.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUBMISSION CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFile: submission_late_fusion.csv\")\n",
    "print(f\"Method: {best_method_name}\")\n",
    "print(f\"Expected Performance (from validation):\")\n",
    "print(f\"  Accuracy: {best_f1:.4f}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_late_fusion['Label'].value_counts())\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_late_fusion.head(10))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1649dd1",
   "metadata": {},
   "source": [
    "### STEP 9: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATE FUSION - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n MODEL ARCHITECTURE:\")\n",
    "print(\"   Model 1: Neural Network (3 hidden layers)\")\n",
    "print(\"           - Input: political_specific_count, political_specific_ratio\")\n",
    "print(\"           - Architecture: 64â†’32â†’16â†’1 neurons\")\n",
    "print(\"   \")\n",
    "print(\"   Model 2: Stacking Ensemble\")\n",
    "print(\"           - Base models: mBERT (text) + CLIP (image)\")\n",
    "print(\"           - Meta-model: Logistic Regression\")\n",
    "print(\"           - Features: 14 engineered features\")\n",
    "\n",
    "print(\"\\n PERFORMANCE PROGRESSION:\")\n",
    "print(f\"   Neural Network:     F1={nn_f1_val:.4f}\")\n",
    "print(f\"   Ensemble:           F1={ensemble_f1:.4f}\")\n",
    "print(f\"   Late Fusion (Best): F1={best_f1:.4f}\")\n",
    "print(f\"   \")\n",
    "print(f\"   Improvement: +{(best_f1 - max(nn_f1_val, ensemble_f1))*100:.2f}%\")\n",
    "\n",
    "print(\"\\n LATE FUSION METHOD:\")\n",
    "print(f\"   Selected: {best_method_name}\")\n",
    "print(f\"   Combines: Neural Network + Stacking Ensemble\")\n",
    "\n",
    "print(\"\\n OUTPUT FILES:\")\n",
    "print(\"   1. submission_late_fusion.csv (FINAL - BEST)\")\n",
    "print(\"   2. late_fusion_comparison.png\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
