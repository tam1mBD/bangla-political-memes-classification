{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5692ffff",
   "metadata": {},
   "source": [
    "# Ensemble Model- Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2f25b",
   "metadata": {},
   "source": [
    "#  Load Pre-trained Models\n",
    "\n",
    "## 1. Text Model (mBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD MBERT MODEL\n",
    "# =============================\n",
    "print(\"Loading mBERT model...\")\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "mbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "mbert_model.load_state_dict(torch.load(MBERT_MODEL_PATH, map_location=device))\n",
    "mbert_model = mbert_model.to(device)\n",
    "mbert_model.eval()\n",
    "\n",
    "print(\"✓ mBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8c297",
   "metadata": {},
   "source": [
    "## 2. Visual Model (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LOAD CLIP MODEL\n",
    "# =============================\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# CLIP Classifier class (same as training)\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, dropout=0.3):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        hidden_size = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = vision_outputs.pooler_output\n",
    "        image_embeds = self.clip.visual_projection(image_embeds)\n",
    "        logits = self.classifier(image_embeds)\n",
    "        return logits\n",
    "\n",
    "# Load CLIP\n",
    "clip_base = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPClassifier(clip_base, num_classes=2)\n",
    "\n",
    "# Load trained weights\n",
    "clip_model.load_state_dict(torch.load(CLIP_MODEL_PATH, map_location=device))\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"✓ CLIP model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ee682",
   "metadata": {},
   "source": [
    "#  Generate Predictions from Base Models\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# PREDICTION FUNCTIONS\n",
    "# =============================\n",
    "\n",
    "def get_text_predictions(texts, model, tokenizer, device, batch_size=16):\n",
    "    \"\"\"Get probability predictions from text model (mBERT)\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Text predictions\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "def get_image_predictions(image_names, img_dir, model, processor, device, batch_size=16):\n",
    "    \"\"\"Get probability predictions from visual model (CLIP)\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(image_names), batch_size), desc=\"Image predictions\"):\n",
    "            batch_names = image_names[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # Load images\n",
    "            for img_name in batch_names:\n",
    "                img_path = os.path.join(img_dir, img_name)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                except:\n",
    "                    img = Image.new('RGB', (224, 224), (128, 128, 128))\n",
    "                batch_images.append(img)\n",
    "            \n",
    "            # Process images\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(pixel_values)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "print(\"Prediction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5508f",
   "metadata": {},
   "source": [
    "## Generate Validation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# GENERATE VALIDATION PREDICTIONS\n",
    "# =============================\n",
    "print(\"Generating predictions on validation set...\\n\")\n",
    "\n",
    "# Text predictions\n",
    "val_texts = val_split['Processed_Text'].fillna('').tolist()\n",
    "text_proba_val = get_text_predictions(val_texts, mbert_model, mbert_tokenizer, device)\n",
    "\n",
    "# Image predictions\n",
    "val_images = val_split['Image_name'].tolist()\n",
    "image_proba_val = get_image_predictions(val_images, TRAIN_IMG_DIR, clip_model, clip_processor, device)\n",
    "\n",
    "# True labels\n",
    "y_val = (val_split['Label'] == 'Political').astype(int).values\n",
    "\n",
    "print(f\"\\nValidation predictions generated!\")\n",
    "print(f\"Text probabilities shape: {text_proba_val.shape}\")\n",
    "print(f\"Image probabilities shape: {image_proba_val.shape}\")\n",
    "print(f\"Labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e6cb7",
   "metadata": {},
   "source": [
    "## Analyze Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# BASE MODEL PERFORMANCE\n",
    "# =============================\n",
    "text_preds_val = np.argmax(text_proba_val, axis=1)\n",
    "image_preds_val = np.argmax(image_proba_val, axis=1)\n",
    "\n",
    "text_acc = accuracy_score(y_val, text_preds_val)\n",
    "text_f1 = f1_score(y_val, text_preds_val, average='macro')\n",
    "\n",
    "image_acc = accuracy_score(y_val, image_preds_val)\n",
    "image_f1 = f1_score(y_val, image_preds_val, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL PERFORMANCE ON VALIDATION SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nText Model (mBERT):\")\n",
    "print(f\"  Accuracy: {text_acc:.4f}\")\n",
    "print(f\"  F1 Score: {text_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nImage Model (CLIP):\")\n",
    "print(f\"  Accuracy: {image_acc:.4f}\")\n",
    "print(f\"  F1 Score: {image_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6add5",
   "metadata": {},
   "source": [
    "#  Engineer Meta-Features for Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# FEATURE ENGINEERING FOR META-MODEL\n",
    "# =============================\n",
    "\n",
    "def create_stacking_features(text_proba, image_proba):\n",
    "    \"\"\"\n",
    "    Create engineered features for meta-model\n",
    "    \n",
    "    Features:\n",
    "    1-2: Raw text probabilities [NonPol, Pol]\n",
    "    3-4: Raw image probabilities [NonPol, Pol]\n",
    "    5: Text confidence (max probability)\n",
    "    6: Image confidence (max probability)\n",
    "    7: Models agreement (binary: do they predict same class?)\n",
    "    8: Disagreement magnitude (absolute difference in Political prob)\n",
    "    9: Both predict Political (product of Political probs)\n",
    "    10: Both predict NonPolitical (product of NonPolitical probs)\n",
    "    11: Average Political probability\n",
    "    12: Max Political probability\n",
    "    13: Min Political probability\n",
    "    14: Difference (Text Political - Image Political)\n",
    "    \"\"\"\n",
    "    features = np.column_stack([\n",
    "        # Base probabilities (4 features)\n",
    "        text_proba[:, 0],              # Text: prob NonPolitical\n",
    "        text_proba[:, 1],              # Text: prob Political\n",
    "        image_proba[:, 0],             # Image: prob NonPolitical\n",
    "        image_proba[:, 1],             # Image: prob Political\n",
    "        \n",
    "        # Confidence features (2 features)\n",
    "        np.max(text_proba, axis=1),    # Text confidence\n",
    "        np.max(image_proba, axis=1),   # Image confidence\n",
    "        \n",
    "        # Agreement features (2 features)\n",
    "        (np.argmax(text_proba, axis=1) == np.argmax(image_proba, axis=1)).astype(float),  # Agreement\n",
    "        np.abs(text_proba[:, 1] - image_proba[:, 1]),  # Disagreement magnitude\n",
    "        \n",
    "        # Interaction features (2 features)\n",
    "        text_proba[:, 1] * image_proba[:, 1],  # Both say Political\n",
    "        text_proba[:, 0] * image_proba[:, 0],  # Both say NonPolitical\n",
    "        \n",
    "        # Statistical features (4 features)\n",
    "        (text_proba[:, 1] + image_proba[:, 1]) / 2,  # Average Political prob\n",
    "        np.maximum(text_proba[:, 1], image_proba[:, 1]),  # Max Political prob\n",
    "        np.minimum(text_proba[:, 1], image_proba[:, 1]),  # Min Political prob\n",
    "        text_proba[:, 1] - image_proba[:, 1],  # Difference (Text - Image)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create validation features\n",
    "X_val_stacking = create_stacking_features(text_proba_val, image_proba_val)\n",
    "\n",
    "print(f\"Stacking features shape: {X_val_stacking.shape}\")\n",
    "print(f\"Total features: {X_val_stacking.shape[1]}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Base probabilities: 4\")\n",
    "print(f\"  - Confidence features: 2\")\n",
    "print(f\"  - Agreement features: 2\")\n",
    "print(f\"  - Interaction features: 2\")\n",
    "print(f\"  - Statistical features: 4\")\n",
    "print(f\"  - Total: 14 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974bd5d0",
   "metadata": {},
   "source": [
    "# Train Meta-Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TRAIN META-MODEL WITH GRID SEARCH\n",
    "# =============================\n",
    "print(\"Training Logistic Regression meta-model with Grid Search...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'class_weight': ['balanced', None],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Create base meta-model\n",
    "meta_model_base = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    meta_model_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_val_stacking, y_val)\n",
    "\n",
    "# Best model\n",
    "meta_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36478202",
   "metadata": {},
   "source": [
    "## Analyze Meta-Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc060a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# META-MODEL PERFORMANCE\n",
    "# =============================\n",
    "ensemble_preds_val = meta_model.predict(X_val_stacking)\n",
    "ensemble_proba_val = meta_model.predict_proba(X_val_stacking)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_val, ensemble_preds_val)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_preds_val, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEnsemble (Stacking):\")\n",
    "print(f\"  Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"  F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nText Model:     Acc={text_acc:.4f}, F1={text_f1:.4f}\")\n",
    "print(f\"Image Model:    Acc={image_acc:.4f}, F1={image_f1:.4f}\")\n",
    "print(f\"Ensemble:       Acc={ensemble_acc:.4f}, F1={ensemble_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMPROVEMENTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy improvement over Text:  {(ensemble_acc - text_acc)*100:+.2f}%\")\n",
    "print(f\"Accuracy improvement over Image: {(ensemble_acc - image_acc)*100:+.2f}%\")\n",
    "print(f\"F1 improvement over Text:        {(ensemble_f1 - text_f1)*100:+.2f}%\")\n",
    "print(f\"F1 improvement over Image:       {(ensemble_f1 - image_f1)*100:+.2f}%\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
